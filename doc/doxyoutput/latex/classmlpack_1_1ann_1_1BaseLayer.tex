\doxysection{Base\+Layer$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$ Class Template Reference}
\label{classmlpack_1_1ann_1_1BaseLayer}\index{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}}


Implementation of the base layer.  


\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\textbf{ Base\+Layer} ()
\begin{DoxyCompactList}\small\item\em Create the \doxyref{Base\+Layer}{p.}{classmlpack_1_1ann_1_1BaseLayer} object. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\void \textbf{ Backward} (const arma\+::\+Mat$<$ eT $>$ \&input, const arma\+::\+Mat$<$ eT $>$ \&gy, arma\+::\+Mat$<$ eT $>$ \&g)
\begin{DoxyCompactList}\small\item\em Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards trough f. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Delta} ()
\begin{DoxyCompactList}\small\item\em Modify the delta. \end{DoxyCompactList}\item 
Output\+Data\+Type const  \& \textbf{ Delta} () const
\begin{DoxyCompactList}\small\item\em Get the delta. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Input\+Type , typename Output\+Type $>$ }\\void \textbf{ Forward} (const Input\+Type \&input, Output\+Type \&output)
\begin{DoxyCompactList}\small\item\em Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Output\+Parameter} ()
\begin{DoxyCompactList}\small\item\em Modify the output parameter. \end{DoxyCompactList}\item 
Output\+Data\+Type const  \& \textbf{ Output\+Parameter} () const
\begin{DoxyCompactList}\small\item\em Get the output parameter. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Archive $>$ }\\void \textbf{ serialize} (Archive \&, const uint32\+\_\+t)
\begin{DoxyCompactList}\small\item\em Serialize the layer. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\subsubsection*{template$<$class Activation\+Function = Logistic\+Function, typename Input\+Data\+Type = arma\+::mat, typename Output\+Data\+Type = arma\+::mat$>$\newline
class mlpack\+::ann\+::\+Base\+Layer$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$}

Implementation of the base layer. 

The base layer works as a metaclass which attaches various functions to the embedding layer.

A few convenience typedefs are given\+:


\begin{DoxyItemize}
\item Sigmoid\+Layer
\item Identity\+Layer
\item Re\+LULayer
\item Tan\+HLayer
\item Softplus\+Layer
\item Hard\+Sigmoid\+Layer
\item Swish\+Layer
\item Mish\+Layer
\item Li\+SHTLayer
\item GELULayer
\item ELi\+SHLayer
\item Elliot\+Layer
\item Gaussian\+Layer
\item Hard\+Swish\+Layer
\item Tanh\+Exp\+Layer
\item SILULayer
\end{DoxyItemize}


\begin{DoxyTemplParams}{Template Parameters}
{\em Activation\+Function} & Activation function used for the embedding layer. \\
\hline
{\em Input\+Data\+Type} & Type of the input data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
{\em Output\+Data\+Type} & Type of the output data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
\end{DoxyTemplParams}


Definition at line 71 of file base\+\_\+layer.\+hpp.



\doxysubsection{Constructor \& Destructor Documentation}
\mbox{\label{classmlpack_1_1ann_1_1BaseLayer_a5efea921257d23b1d5a755609d7a7943}} 
\index{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}!BaseLayer@{BaseLayer}}
\index{BaseLayer@{BaseLayer}!BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}}
\doxysubsubsection{BaseLayer()}
{\footnotesize\ttfamily \textbf{ Base\+Layer} (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Create the \doxyref{Base\+Layer}{p.}{classmlpack_1_1ann_1_1BaseLayer} object. 



Definition at line 77 of file base\+\_\+layer.\+hpp.



\doxysubsection{Member Function Documentation}
\mbox{\label{classmlpack_1_1ann_1_1BaseLayer_a78dbad83871f43db1975e45a9a69c376}} 
\index{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}!Backward@{Backward}}
\index{Backward@{Backward}!BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}}
\doxysubsubsection{Backward()}
{\footnotesize\ttfamily void Backward (\begin{DoxyParamCaption}\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{input,  }\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{gy,  }\item[{arma\+::\+Mat$<$ eT $>$ \&}]{g }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards trough f. 

Using the results from the feed forward pass.


\begin{DoxyParams}{Parameters}
{\em input} & The propagated input activation. \\
\hline
{\em gy} & The backpropagated error. \\
\hline
{\em g} & The calculated gradient. \\
\hline
\end{DoxyParams}


Definition at line 105 of file base\+\_\+layer.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1BaseLayer_ad6601342d560219ce951d554e69e5e87}} 
\index{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}!Delta@{Delta}}
\index{Delta@{Delta}!BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}}
\doxysubsubsection{Delta()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Delta (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the delta. 



Definition at line 122 of file base\+\_\+layer.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1BaseLayer_a797f7edb44dd081e5e2b3cc316eef6bd}} 
\index{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}!Delta@{Delta}}
\index{Delta@{Delta}!BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}}
\doxysubsubsection{Delta()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Output\+Data\+Type const\& Delta (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the delta. 



Definition at line 120 of file base\+\_\+layer.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1BaseLayer_a09440df0a90bdcc766e56e097d91205b}} 
\index{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}!Forward@{Forward}}
\index{Forward@{Forward}!BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}}
\doxysubsubsection{Forward()}
{\footnotesize\ttfamily void Forward (\begin{DoxyParamCaption}\item[{const Input\+Type \&}]{input,  }\item[{Output\+Type \&}]{output }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f. 


\begin{DoxyParams}{Parameters}
{\em input} & Input data used for evaluating the specified function. \\
\hline
{\em output} & Resulting output activation. \\
\hline
\end{DoxyParams}


Definition at line 90 of file base\+\_\+layer.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1BaseLayer_a21d5f745f02c709625a4ee0907f004a5}} 
\index{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}!OutputParameter@{OutputParameter}}
\index{OutputParameter@{OutputParameter}!BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}}
\doxysubsubsection{OutputParameter()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Output\+Parameter (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the output parameter. 



Definition at line 117 of file base\+\_\+layer.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1BaseLayer_a0ee21c2a36e5abad1e7a9d5dd00849f9}} 
\index{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}!OutputParameter@{OutputParameter}}
\index{OutputParameter@{OutputParameter}!BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}}
\doxysubsubsection{OutputParameter()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Output\+Data\+Type const\& Output\+Parameter (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the output parameter. 



Definition at line 115 of file base\+\_\+layer.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1BaseLayer_aa2ccb5a0533a6ba0abe6dfc1f98fbafb}} 
\index{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}!serialize@{serialize}}
\index{serialize@{serialize}!BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$@{BaseLayer$<$ ActivationFunction, InputDataType, OutputDataType $>$}}
\doxysubsubsection{serialize()}
{\footnotesize\ttfamily void serialize (\begin{DoxyParamCaption}\item[{Archive \&}]{,  }\item[{const uint32\+\_\+t}]{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Serialize the layer. 



Definition at line 128 of file base\+\_\+layer.\+hpp.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/aakash/mlpack/src/mlpack/methods/ann/layer/\textbf{ base\+\_\+layer.\+hpp}\end{DoxyCompactItemize}
