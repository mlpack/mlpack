\doxysection{Softmax\+Error\+Function$<$ Metric\+Type $>$ Class Template Reference}
\label{classmlpack_1_1nca_1_1SoftmaxErrorFunction}\index{SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}}


The \char`\"{}softmax\char`\"{} stochastic neighbor assignment probability function.  




Inheritance diagram for Softmax\+Error\+Function$<$ Metric\+Type $>$\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=286pt]{classmlpack_1_1nca_1_1SoftmaxErrorFunction__inherit__graph}
\end{center}
\end{figure}
\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\textbf{ Softmax\+Error\+Function} (const arma\+::mat \&dataset, const arma\+::\+Row$<$ size\+\_\+t $>$ \&labels, Metric\+Type metric=Metric\+Type())
\begin{DoxyCompactList}\small\item\em Initialize with the given kernel; useful when the kernel has some state to store, which is set elsewhere. \end{DoxyCompactList}\item 
double \textbf{ Evaluate} (const arma\+::mat \&covariance)
\begin{DoxyCompactList}\small\item\em Evaluate the softmax function for the given covariance matrix. \end{DoxyCompactList}\item 
double \textbf{ Evaluate} (const arma\+::mat \&covariance, const size\+\_\+t begin, const size\+\_\+t batch\+Size=1)
\begin{DoxyCompactList}\small\item\em Evaluate the softmax objective function for the given covariance matrix on the given batch size from a given inital point of the dataset. \end{DoxyCompactList}\item 
const arma\+::mat \textbf{ Get\+Initial\+Point} () const
\begin{DoxyCompactList}\small\item\em Get the initial point. \end{DoxyCompactList}\item 
void \textbf{ Gradient} (const arma\+::mat \&covariance, arma\+::mat \&gradient)
\begin{DoxyCompactList}\small\item\em Evaluate the gradient of the softmax function for the given covariance matrix. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Grad\+Type $>$ }\\void \textbf{ Gradient} (const arma\+::mat \&covariance, const size\+\_\+t begin, Grad\+Type \&gradient, const size\+\_\+t batch\+Size=1)
\begin{DoxyCompactList}\small\item\em Evaluate the gradient of the softmax function for the given covariance matrix on the given batch size, from a given initial point of the dataset. \end{DoxyCompactList}\item 
size\+\_\+t \textbf{ Num\+Functions} () const
\begin{DoxyCompactList}\small\item\em Get the number of functions the objective function can be decomposed into. \end{DoxyCompactList}\item 
void \textbf{ Shuffle} ()
\begin{DoxyCompactList}\small\item\em Shuffle the dataset. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\subsubsection*{template$<$typename Metric\+Type = metric\+::\+Squared\+Euclidean\+Distance$>$\newline
class mlpack\+::nca\+::\+Softmax\+Error\+Function$<$ Metric\+Type $>$}

The \char`\"{}softmax\char`\"{} stochastic neighbor assignment probability function. 

The actual function is

p\+\_\+ij = (exp(-\/$\vert$$\vert$ A x\+\_\+i -\/ A x\+\_\+j $\vert$$\vert$ $^\wedge$ 2)) / (sum\+\_\+\{k != i\} (exp(-\/$\vert$$\vert$ A x\+\_\+i -\/ A x\+\_\+k $\vert$$\vert$ $^\wedge$ 2)))

where x\+\_\+n represents a point and A is the current scaling matrix.

This class is more flexible than the original paper, allowing an arbitrary metric function to be used in place of $\vert$$\vert$ A x\+\_\+i -\/ A x\+\_\+j $\vert$$\vert$$^\wedge$2, meaning that the squared Euclidean distance is not the only allowed metric for \doxyref{NCA}{p.}{classmlpack_1_1nca_1_1NCA}. However, that is probably the best way to use this class.

In addition to the standard \doxyref{Evaluate()}{p.}{classmlpack_1_1nca_1_1SoftmaxErrorFunction_a7bf1f5613d393e7b885ed44660ca1069} and \doxyref{Gradient()}{p.}{classmlpack_1_1nca_1_1SoftmaxErrorFunction_a8464e351c740da7124aaa748e171ce4b} functions which mlpack optimizers use, overloads of \doxyref{Evaluate()}{p.}{classmlpack_1_1nca_1_1SoftmaxErrorFunction_a7bf1f5613d393e7b885ed44660ca1069} and \doxyref{Gradient()}{p.}{classmlpack_1_1nca_1_1SoftmaxErrorFunction_a8464e351c740da7124aaa748e171ce4b} are given which only operate on one point in the dataset. This is useful for optimizers like stochastic gradient descent (see mlpack\+::optimization\+::\+SGD). 

Definition at line 45 of file nca\+\_\+softmax\+\_\+error\+\_\+function.\+hpp.



\doxysubsection{Constructor \& Destructor Documentation}
\mbox{\label{classmlpack_1_1nca_1_1SoftmaxErrorFunction_aed757d2c1bfdc680098a2544e3eae97e}} 
\index{SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}!SoftmaxErrorFunction@{SoftmaxErrorFunction}}
\index{SoftmaxErrorFunction@{SoftmaxErrorFunction}!SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}}
\doxysubsubsection{SoftmaxErrorFunction()}
{\footnotesize\ttfamily \textbf{ Softmax\+Error\+Function} (\begin{DoxyParamCaption}\item[{const arma\+::mat \&}]{dataset,  }\item[{const arma\+::\+Row$<$ size\+\_\+t $>$ \&}]{labels,  }\item[{Metric\+Type}]{metric = {\ttfamily MetricType()} }\end{DoxyParamCaption})}



Initialize with the given kernel; useful when the kernel has some state to store, which is set elsewhere. 

If no kernel is given, an empty kernel is used; this way, you can call the constructor with no arguments. A reference to the dataset we will be optimizing over is also required.


\begin{DoxyParams}{Parameters}
{\em dataset} & Matrix containing the dataset. \\
\hline
{\em labels} & Vector of class labels for each point in the dataset. \\
\hline
{\em metric} & Instantiated metric (optional). \\
\hline
\end{DoxyParams}


\doxysubsection{Member Function Documentation}
\mbox{\label{classmlpack_1_1nca_1_1SoftmaxErrorFunction_a7bf1f5613d393e7b885ed44660ca1069}} 
\index{SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}!Evaluate@{Evaluate}}
\index{Evaluate@{Evaluate}!SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}}
\doxysubsubsection{Evaluate()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily double Evaluate (\begin{DoxyParamCaption}\item[{const arma\+::mat \&}]{covariance }\end{DoxyParamCaption})}



Evaluate the softmax function for the given covariance matrix. 

This is the non-\/separable implementation, where the objective function is not decomposed into the sum of several objective functions.


\begin{DoxyParams}{Parameters}
{\em covariance} & Covariance matrix of Mahalanobis distance. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1nca_1_1SoftmaxErrorFunction_a4229a84a3552879f556421f3084a8269}} 
\index{SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}!Evaluate@{Evaluate}}
\index{Evaluate@{Evaluate}!SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}}
\doxysubsubsection{Evaluate()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily double Evaluate (\begin{DoxyParamCaption}\item[{const arma\+::mat \&}]{covariance,  }\item[{const size\+\_\+t}]{begin,  }\item[{const size\+\_\+t}]{batch\+Size = {\ttfamily 1} }\end{DoxyParamCaption})}



Evaluate the softmax objective function for the given covariance matrix on the given batch size from a given inital point of the dataset. 

This is the separable implementation, where the objective function is decomposed into the sum of many objective functions, and here, only one of those constituent objective functions is returned.


\begin{DoxyParams}{Parameters}
{\em covariance} & Covariance matrix of Mahalanobis distance. \\
\hline
{\em begin} & Index of the initial point to use for objective function. \\
\hline
{\em batch\+Size} & Number of points to use for objective function. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1nca_1_1SoftmaxErrorFunction_a595b81d2593c99409f31f524fe12bfcf}} 
\index{SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}!GetInitialPoint@{GetInitialPoint}}
\index{GetInitialPoint@{GetInitialPoint}!SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}}
\doxysubsubsection{GetInitialPoint()}
{\footnotesize\ttfamily const arma\+::mat Get\+Initial\+Point (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const}



Get the initial point. 

\mbox{\label{classmlpack_1_1nca_1_1SoftmaxErrorFunction_a8464e351c740da7124aaa748e171ce4b}} 
\index{SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}}
\doxysubsubsection{Gradient()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily void Gradient (\begin{DoxyParamCaption}\item[{const arma\+::mat \&}]{covariance,  }\item[{arma\+::mat \&}]{gradient }\end{DoxyParamCaption})}



Evaluate the gradient of the softmax function for the given covariance matrix. 

This is the non-\/separable implementation, where the objective function is not decomposed into the sum of several objective functions.


\begin{DoxyParams}{Parameters}
{\em covariance} & Covariance matrix of Mahalanobis distance. \\
\hline
{\em gradient} & Matrix to store the calculated gradient in. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1nca_1_1SoftmaxErrorFunction_a8dc4cf443bea9b2edaef1d437b111ffe}} 
\index{SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}}
\doxysubsubsection{Gradient()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily void Gradient (\begin{DoxyParamCaption}\item[{const arma\+::mat \&}]{covariance,  }\item[{const size\+\_\+t}]{begin,  }\item[{Grad\+Type \&}]{gradient,  }\item[{const size\+\_\+t}]{batch\+Size = {\ttfamily 1} }\end{DoxyParamCaption})}



Evaluate the gradient of the softmax function for the given covariance matrix on the given batch size, from a given initial point of the dataset. 

This is the separable implementation, where the objective function is decomposed into the sum of many objective functions, and here, only one of those constituent objective functions is returned. The type of the gradient parameter is a template argument to allow the computation of a sparse gradient.


\begin{DoxyTemplParams}{Template Parameters}
{\em Grad\+Type} & The type of the gradient out-\/param. \\
\hline
\end{DoxyTemplParams}

\begin{DoxyParams}{Parameters}
{\em covariance} & Covariance matrix of Mahalanobis distance. \\
\hline
{\em begin} & Index of the initial point to use for objective function. \\
\hline
{\em batch\+Size} & Number of points to use for objective function. \\
\hline
{\em gradient} & Matrix to store the calculated gradient in. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1nca_1_1SoftmaxErrorFunction_a1fa76af34a6e3ea927b307f0c318ee4b}} 
\index{SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}!NumFunctions@{NumFunctions}}
\index{NumFunctions@{NumFunctions}!SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}}
\doxysubsubsection{NumFunctions()}
{\footnotesize\ttfamily size\+\_\+t Num\+Functions (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the number of functions the objective function can be decomposed into. 

This is just the number of points in the dataset. 

Definition at line 132 of file nca\+\_\+softmax\+\_\+error\+\_\+function.\+hpp.

\mbox{\label{classmlpack_1_1nca_1_1SoftmaxErrorFunction_a2697cc8b37d7bca7c055228382a9b208}} 
\index{SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}!Shuffle@{Shuffle}}
\index{Shuffle@{Shuffle}!SoftmaxErrorFunction$<$ MetricType $>$@{SoftmaxErrorFunction$<$ MetricType $>$}}
\doxysubsubsection{Shuffle()}
{\footnotesize\ttfamily void Shuffle (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Shuffle the dataset. 



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/aakash/mlpack/src/mlpack/methods/nca/\textbf{ nca\+\_\+softmax\+\_\+error\+\_\+function.\+hpp}\end{DoxyCompactItemize}
