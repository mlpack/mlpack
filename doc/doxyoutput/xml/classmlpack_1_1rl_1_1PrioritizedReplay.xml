<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.9.1" xml:lang="en-US">
  <compounddef id="classmlpack_1_1rl_1_1PrioritizedReplay" kind="class" language="C++" prot="public">
    <compoundname>mlpack::rl::PrioritizedReplay</compoundname>
    <includes refid="prioritized__replay_8hpp" local="no">prioritized_replay.hpp</includes>
    <innerclass refid="structmlpack_1_1rl_1_1PrioritizedReplay_1_1Transition" prot="public">mlpack::rl::PrioritizedReplay::Transition</innerclass>
    <templateparamlist>
      <param>
        <type>typename EnvironmentType</type>
      </param>
    </templateparamlist>
      <sectiondef kind="public-type">
      <memberdef kind="typedef" id="classmlpack_1_1rl_1_1PrioritizedReplay_1aaf7b2dc5d49d01961601c7c16be76777" prot="public" static="no">
        <type>typename EnvironmentType::Action</type>
        <definition>using ActionType =  typename EnvironmentType::Action</definition>
        <argsstring></argsstring>
        <name>ActionType</name>
        <briefdescription>
<para>Convenient typedef for action. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="43" column="3" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="43" bodyend="-1"/>
      </memberdef>
      <memberdef kind="typedef" id="classmlpack_1_1rl_1_1PrioritizedReplay_1ada68ef405b7c331a2bee337614f00088" prot="public" static="no">
        <type>typename EnvironmentType::State</type>
        <definition>using StateType =  typename EnvironmentType::State</definition>
        <argsstring></argsstring>
        <name>StateType</name>
        <briefdescription>
<para>Convenient typedef for state. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="46" column="3" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="46" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="private-attrib">
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a06e5cf57388e6c4657085808d2e78deb" prot="private" static="no" mutable="no">
        <type>std::vector&lt; <ref refid="classmlpack_1_1rl_1_1PrioritizedReplay_1aaf7b2dc5d49d01961601c7c16be76777" kindref="member">ActionType</ref> &gt;</type>
        <definition>std::vector&lt;ActionType&gt; actions</definition>
        <argsstring></argsstring>
        <name>actions</name>
        <briefdescription>
<para>Locally-stored previous actions. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="358" column="15" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="358" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a20f4c4490bc8ecbdd1ffcb79acce6035" prot="private" static="no" mutable="no">
        <type>double</type>
        <definition>double alpha</definition>
        <argsstring></argsstring>
        <name>alpha</name>
        <briefdescription>
<para>How much prioritization is used. </para>
        </briefdescription>
        <detaileddescription>
<para>(0 - no prioritization, 1 - full prioritization) </para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="325" column="10" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="325" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1acc8b6b6879b002e34f544046de374ce8" prot="private" static="no" mutable="no">
        <type>size_t</type>
        <definition>size_t batchSize</definition>
        <argsstring></argsstring>
        <name>batchSize</name>
        <briefdescription>
<para>Locally-stored number of examples of each sample. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="312" column="10" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="312" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a9424343761f8c4f4c1afe8f5b6bf471b" prot="private" static="no" mutable="no">
        <type>double</type>
        <definition>double beta</definition>
        <argsstring></argsstring>
        <name>beta</name>
        <briefdescription>
<para>The value of beta for current sample. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="334" column="10" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="334" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1ad721fc6ca6a3d6ba3bc506576622aab0" prot="private" static="no" mutable="no">
        <type>size_t</type>
        <definition>size_t capacity</definition>
        <argsstring></argsstring>
        <name>capacity</name>
        <briefdescription>
<para>Locally-stored total memory limit. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="315" column="10" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="315" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a19f31d9744c12503a9cbdaa1fd3a9848" prot="private" static="no" mutable="no">
        <type>bool</type>
        <definition>bool full</definition>
        <argsstring></argsstring>
        <name>full</name>
        <briefdescription>
<para>Locally-stored indicator that whether the memory is full or not. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="321" column="8" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="321" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a690828f336f65c5372058811d4425337" prot="private" static="no" mutable="no">
        <type><ref refid="classmlpack_1_1rl_1_1SumTree" kindref="compound">SumTree</ref>&lt; double &gt;</type>
        <definition>SumTree&lt;double&gt; idxSum</definition>
        <argsstring></argsstring>
        <name>idxSum</name>
        <briefdescription>
<para>Locally-stored the prefix sum of prioritization. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="340" column="11" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="340" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a684edff0823aece7d245190fa8ffa2f3" prot="private" static="no" mutable="no">
        <type>double</type>
        <definition>double initialBeta</definition>
        <argsstring></argsstring>
        <name>initialBeta</name>
        <briefdescription>
<para>Initial value of beta for prioritized replay buffer. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="331" column="10" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="331" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a76e2d6022b6c86f6365a2d6833840370" prot="private" static="no" mutable="no">
        <type>arma::irowvec</type>
        <definition>arma::irowvec isTerminal</definition>
        <argsstring></argsstring>
        <name>isTerminal</name>
        <briefdescription>
<para>Locally-stored termination information of previous experience. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="367" column="17" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="367" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a0bd142a7ea09a9d1070a4c3af7a355c2" prot="private" static="no" mutable="no">
        <type>double</type>
        <definition>double maxPriority</definition>
        <argsstring></argsstring>
        <name>maxPriority</name>
        <briefdescription>
<para>Locally-stored the max priority. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="328" column="10" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="328" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a0789a262d579655e5853c5d86e5b7632" prot="private" static="no" mutable="no">
        <type>arma::mat</type>
        <definition>arma::mat nextStates</definition>
        <argsstring></argsstring>
        <name>nextStates</name>
        <briefdescription>
<para>Locally-stored encoded previous next states. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="364" column="13" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="364" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1ada94392db6b18c65cb4406977697d429" prot="private" static="no" mutable="no">
        <type>std::deque&lt; <ref refid="structmlpack_1_1rl_1_1PrioritizedReplay_1_1Transition" kindref="compound">Transition</ref> &gt;</type>
        <definition>std::deque&lt;Transition&gt; nStepBuffer</definition>
        <argsstring></argsstring>
        <name>nStepBuffer</name>
        <briefdescription>
<para>Locally-stored buffer containing n consecutive steps. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="352" column="14" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="352" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1ae87d0d3fe7950c32ba7d8f2715e84915" prot="private" static="no" mutable="no">
        <type>size_t</type>
        <definition>size_t nSteps</definition>
        <argsstring></argsstring>
        <name>nSteps</name>
        <briefdescription>
<para>Locally-stored number of steps to look into the future. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="349" column="10" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="349" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a7a04afe5347934be732ec70a70bd0a28" prot="private" static="no" mutable="no">
        <type>size_t</type>
        <definition>size_t position</definition>
        <argsstring></argsstring>
        <name>position</name>
        <briefdescription>
<para>Indicate the position to store new transition. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="318" column="10" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="318" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a052b0363ff8606d45496f2d5016ac4b3" prot="private" static="no" mutable="no">
        <type>size_t</type>
        <definition>size_t replayBetaIters</definition>
        <argsstring></argsstring>
        <name>replayBetaIters</name>
        <briefdescription>
<para>How many iteration for replay beta to decay. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="337" column="10" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="337" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a6c7dee98c84287c38d7ef6b8cc630dfd" prot="private" static="no" mutable="no">
        <type>arma::rowvec</type>
        <definition>arma::rowvec rewards</definition>
        <argsstring></argsstring>
        <name>rewards</name>
        <briefdescription>
<para>Locally-stored previous rewards. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="361" column="16" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="361" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1aeaa586e11d5a59a23ba5689667c447a6" prot="private" static="no" mutable="no">
        <type>arma::ucolvec</type>
        <definition>arma::ucolvec sampledIndices</definition>
        <argsstring></argsstring>
        <name>sampledIndices</name>
        <briefdescription>
<para>Locally-stored the indices of sampled transitions. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="343" column="17" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="343" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a6a63dc9a374f1be6a4166445905c6c80" prot="private" static="no" mutable="no">
        <type>arma::mat</type>
        <definition>arma::mat states</definition>
        <argsstring></argsstring>
        <name>states</name>
        <briefdescription>
<para>Locally-stored encoded previous states. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="355" column="13" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="355" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a36b9d4671b1f33385b9b80c2d654d5ff" prot="private" static="no" mutable="no">
        <type>arma::rowvec</type>
        <definition>arma::rowvec weights</definition>
        <argsstring></argsstring>
        <name>weights</name>
        <briefdescription>
<para>Locally-stored the weights of sampled transitions. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="346" column="16" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="346" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="public-func">
      <memberdef kind="function" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a2d2ee6b689ad5f996c939be2f1f61ba0" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type></type>
        <definition>PrioritizedReplay</definition>
        <argsstring>()</argsstring>
        <name>PrioritizedReplay</name>
        <briefdescription>
<para>Default constructor. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="60" column="3" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="60" bodyend="71"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a2754147b888db0a76084538e3426c4f9" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type></type>
        <definition>PrioritizedReplay</definition>
        <argsstring>(const size_t batchSize, const size_t capacity, const double alpha, const size_t nSteps=1, const size_t dimension=StateType::dimension)</argsstring>
        <name>PrioritizedReplay</name>
        <param>
          <type>const size_t</type>
          <declname>batchSize</declname>
        </param>
        <param>
          <type>const size_t</type>
          <declname>capacity</declname>
        </param>
        <param>
          <type>const double</type>
          <declname>alpha</declname>
        </param>
        <param>
          <type>const size_t</type>
          <declname>nSteps</declname>
          <defval>1</defval>
        </param>
        <param>
          <type>const size_t</type>
          <declname>dimension</declname>
          <defval>StateType::dimension</defval>
        </param>
        <briefdescription>
<para>Construct an instance of prioritized experience replay class. </para>
        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>batchSize</parametername>
</parameternamelist>
<parameterdescription>
<para>Number of examples returned at each sample. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>capacity</parametername>
</parameternamelist>
<parameterdescription>
<para>Total memory size in terms of number of examples. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>alpha</parametername>
</parameternamelist>
<parameterdescription>
<para>How much prioritization is used. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>nSteps</parametername>
</parameternamelist>
<parameterdescription>
<para>Number of steps to look in the future. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>dimension</parametername>
</parameternamelist>
<parameterdescription>
<para>The dimension of an encoded state. </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="82" column="3" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="82" bodyend="110"/>
        <references refid="det_8txt_1ae7ed598bdef2a989db6a7ae6e7c11288" compoundref="det_8txt" startline="344" endline="367">alpha</references>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a26967aa9c873e7085b621d541d4120e0" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>void</type>
        <definition>void BetaAnneal</definition>
        <argsstring>()</argsstring>
        <name>BetaAnneal</name>
        <briefdescription>
<para>Annealing the beta. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="276" column="8" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="276" bodyend="279"/>
        <referencedby refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a6ecc6da2d5f83f0eefdc74be3465925a" compoundref="prioritized__replay_8hpp" startline="221" endline="248">PrioritizedReplay&lt; EnvironmentType &gt;::Sample</referencedby>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1PrioritizedReplay_1abf36129b66f5b30a65d96d11ebfde027" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>void</type>
        <definition>void GetNStepInfo</definition>
        <argsstring>(double &amp;reward, StateType &amp;nextState, bool &amp;isEnd, const double &amp;discount)</argsstring>
        <name>GetNStepInfo</name>
        <param>
          <type>double &amp;</type>
          <declname>reward</declname>
        </param>
        <param>
          <type><ref refid="classmlpack_1_1rl_1_1PrioritizedReplay_1ada68ef405b7c331a2bee337614f00088" kindref="member">StateType</ref> &amp;</type>
          <declname>nextState</declname>
        </param>
        <param>
          <type>bool &amp;</type>
          <declname>isEnd</declname>
        </param>
        <param>
          <type>const double &amp;</type>
          <declname>discount</declname>
        </param>
        <briefdescription>
<para>Get the reward, next state and terminal boolean for nth step. </para>
        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>reward</parametername>
</parameternamelist>
<parameterdescription>
<para>Given reward. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>nextState</parametername>
</parameternamelist>
<parameterdescription>
<para>Given next state. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>isEnd</parametername>
</parameternamelist>
<parameterdescription>
<para>Whether next state is terminal state. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>discount</parametername>
</parameternamelist>
<parameterdescription>
<para>The discount parameter. </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="171" column="8" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="171" bodyend="191"/>
        <referencedby refid="classmlpack_1_1rl_1_1PrioritizedReplay_1ab17ee90540cf7b26647b57acf16116d5" compoundref="prioritized__replay_8hpp" startline="122" endline="161">PrioritizedReplay&lt; EnvironmentType &gt;::Store</referencedby>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a48a86a6254329a98e1f15d4722c4e85b" prot="public" static="no" const="yes" explicit="no" inline="yes" virt="non-virtual">
        <type>const size_t &amp;</type>
        <definition>const size_t&amp; NSteps</definition>
        <argsstring>() const</argsstring>
        <name>NSteps</name>
        <briefdescription>
<para>Get the number of steps for n-step agent. </para>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="308" column="16" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="308" bodyend="308"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a6ecc6da2d5f83f0eefdc74be3465925a" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>void</type>
        <definition>void Sample</definition>
        <argsstring>(arma::mat &amp;sampledStates, std::vector&lt; ActionType &gt; &amp;sampledActions, arma::rowvec &amp;sampledRewards, arma::mat &amp;sampledNextStates, arma::irowvec &amp;isTerminal)</argsstring>
        <name>Sample</name>
        <param>
          <type>arma::mat &amp;</type>
          <declname>sampledStates</declname>
        </param>
        <param>
          <type>std::vector&lt; <ref refid="classmlpack_1_1rl_1_1PrioritizedReplay_1aaf7b2dc5d49d01961601c7c16be76777" kindref="member">ActionType</ref> &gt; &amp;</type>
          <declname>sampledActions</declname>
        </param>
        <param>
          <type>arma::rowvec &amp;</type>
          <declname>sampledRewards</declname>
        </param>
        <param>
          <type>arma::mat &amp;</type>
          <declname>sampledNextStates</declname>
        </param>
        <param>
          <type>arma::irowvec &amp;</type>
          <declname>isTerminal</declname>
        </param>
        <briefdescription>
<para>Sample some experience according to their priorities. </para>
        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>sampledStates</parametername>
</parameternamelist>
<parameterdescription>
<para>Sampled encoded states. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>sampledActions</parametername>
</parameternamelist>
<parameterdescription>
<para>Sampled actions. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>sampledRewards</parametername>
</parameternamelist>
<parameterdescription>
<para>Sampled rewards. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>sampledNextStates</parametername>
</parameternamelist>
<parameterdescription>
<para>Sampled encoded next states. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>isTerminal</parametername>
</parameternamelist>
<parameterdescription>
<para>Indicate whether corresponding next state is terminal state. </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="221" column="8" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="221" bodyend="248"/>
        <references refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a26967aa9c873e7085b621d541d4120e0" compoundref="prioritized__replay_8hpp" startline="276" endline="279">PrioritizedReplay&lt; EnvironmentType &gt;::BetaAnneal</references>
        <references refid="classmlpack_1_1rl_1_1SumTree_1a0f6adb7068497f76d7f01870c3fa32a0" compoundref="sumtree_8hpp" startline="93" endline="97">SumTree&lt; T &gt;::Get</references>
        <references refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a1a45c1e17aad599a64fa6f941979ad10" compoundref="prioritized__replay_8hpp" startline="198" endline="209">PrioritizedReplay&lt; EnvironmentType &gt;::SampleProportional</references>
        <references refid="classmlpack_1_1rl_1_1SumTree_1ae64a6b8188bfdeecb0f6cc3ff2391528" compoundref="sumtree_8hpp" startline="143" endline="147">SumTree&lt; T &gt;::Sum</references>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a1a45c1e17aad599a64fa6f941979ad10" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>arma::ucolvec</type>
        <definition>arma::ucolvec SampleProportional</definition>
        <argsstring>()</argsstring>
        <name>SampleProportional</name>
        <briefdescription>
<para>Sample some experience according to their priorities. </para>
        </briefdescription>
        <detaileddescription>
<para><simplesect kind="return"><para>The indices to be chosen. </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="198" column="17" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="198" bodyend="209"/>
        <references refid="classmlpack_1_1rl_1_1SumTree_1a3cc70bfa6c4dd50a0dd99da3a60ec761" compoundref="sumtree_8hpp" startline="163" endline="179">SumTree&lt; T &gt;::FindPrefixSum</references>
        <references refid="classmlpack_1_1rl_1_1SumTree_1ae64a6b8188bfdeecb0f6cc3ff2391528" compoundref="sumtree_8hpp" startline="143" endline="147">SumTree&lt; T &gt;::Sum</references>
        <referencedby refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a6ecc6da2d5f83f0eefdc74be3465925a" compoundref="prioritized__replay_8hpp" startline="221" endline="248">PrioritizedReplay&lt; EnvironmentType &gt;::Sample</referencedby>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1PrioritizedReplay_1ab8983dc8f7847b4c77148b86d0e7fc8d" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>const size_t &amp;</type>
        <definition>const size_t&amp; Size</definition>
        <argsstring>()</argsstring>
        <name>Size</name>
        <briefdescription>
<para>Get the number of transitions in the memory. </para>
        </briefdescription>
        <detaileddescription>
<para><simplesect kind="return"><para>Actual used memory size. </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="268" column="16" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="268" bodyend="271"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1PrioritizedReplay_1ab17ee90540cf7b26647b57acf16116d5" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>void</type>
        <definition>void Store</definition>
        <argsstring>(StateType state, ActionType action, double reward, StateType nextState, bool isEnd, const double &amp;discount)</argsstring>
        <name>Store</name>
        <param>
          <type><ref refid="classmlpack_1_1rl_1_1PrioritizedReplay_1ada68ef405b7c331a2bee337614f00088" kindref="member">StateType</ref></type>
          <declname>state</declname>
        </param>
        <param>
          <type><ref refid="classmlpack_1_1rl_1_1PrioritizedReplay_1aaf7b2dc5d49d01961601c7c16be76777" kindref="member">ActionType</ref></type>
          <declname>action</declname>
        </param>
        <param>
          <type>double</type>
          <declname>reward</declname>
        </param>
        <param>
          <type><ref refid="classmlpack_1_1rl_1_1PrioritizedReplay_1ada68ef405b7c331a2bee337614f00088" kindref="member">StateType</ref></type>
          <declname>nextState</declname>
        </param>
        <param>
          <type>bool</type>
          <declname>isEnd</declname>
        </param>
        <param>
          <type>const double &amp;</type>
          <declname>discount</declname>
        </param>
        <briefdescription>
<para>Store the given experience and set the priorities for the given experience. </para>
        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>state</parametername>
</parameternamelist>
<parameterdescription>
<para>Given state. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>action</parametername>
</parameternamelist>
<parameterdescription>
<para>Given action. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>reward</parametername>
</parameternamelist>
<parameterdescription>
<para>Given reward. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>nextState</parametername>
</parameternamelist>
<parameterdescription>
<para>Given next state. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>isEnd</parametername>
</parameternamelist>
<parameterdescription>
<para>Whether next state is terminal state. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>discount</parametername>
</parameternamelist>
<parameterdescription>
<para>The discount parameter. </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="122" column="8" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="122" bodyend="161"/>
        <references refid="classmlpack_1_1rl_1_1PrioritizedReplay_1abf36129b66f5b30a65d96d11ebfde027" compoundref="prioritized__replay_8hpp" startline="171" endline="191">PrioritizedReplay&lt; EnvironmentType &gt;::GetNStepInfo</references>
        <references refid="classmlpack_1_1rl_1_1SumTree_1a60cd481b36347a9062b4e0c740c836ec" compoundref="sumtree_8hpp" startline="57" endline="67">SumTree&lt; T &gt;::Set</references>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a9f4cfe4f0266408c291e51db0606b8e0" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>void</type>
        <definition>void Update</definition>
        <argsstring>(arma::mat target, std::vector&lt; ActionType &gt; sampledActions, arma::mat nextActionValues, arma::mat &amp;gradients)</argsstring>
        <name>Update</name>
        <param>
          <type>arma::mat</type>
          <declname>target</declname>
        </param>
        <param>
          <type>std::vector&lt; <ref refid="classmlpack_1_1rl_1_1PrioritizedReplay_1aaf7b2dc5d49d01961601c7c16be76777" kindref="member">ActionType</ref> &gt;</type>
          <declname>sampledActions</declname>
        </param>
        <param>
          <type>arma::mat</type>
          <declname>nextActionValues</declname>
        </param>
        <param>
          <type>arma::mat &amp;</type>
          <declname>gradients</declname>
        </param>
        <briefdescription>
<para>Update the priorities of transitions and Update the gradients. </para>
        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>target</parametername>
</parameternamelist>
<parameterdescription>
<para>The learned value. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>sampledActions</parametername>
</parameternamelist>
<parameterdescription>
<para>Agent&apos;s sampled action. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>nextActionValues</parametername>
</parameternamelist>
<parameterdescription>
<para>Agent&apos;s next action. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>gradients</parametername>
</parameternamelist>
<parameterdescription>
<para>The model&apos;s gradients. </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="289" column="8" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="289" bodyend="305"/>
        <references refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a3b512739fae1601beafbb89d51c40a7d" compoundref="prioritized__replay_8hpp" startline="256" endline="261">PrioritizedReplay&lt; EnvironmentType &gt;::UpdatePriorities</references>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1PrioritizedReplay_1a3b512739fae1601beafbb89d51c40a7d" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>void</type>
        <definition>void UpdatePriorities</definition>
        <argsstring>(arma::ucolvec &amp;indices, arma::colvec &amp;priorities)</argsstring>
        <name>UpdatePriorities</name>
        <param>
          <type>arma::ucolvec &amp;</type>
          <declname>indices</declname>
        </param>
        <param>
          <type>arma::colvec &amp;</type>
          <declname>priorities</declname>
        </param>
        <briefdescription>
<para>Update priorities of sampled transitions. </para>
        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>indices</parametername>
</parameternamelist>
<parameterdescription>
<para>The indices of sample to be updated. </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>priorities</parametername>
</parameternamelist>
<parameterdescription>
<para>Their corresponding priorities. </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="256" column="8" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="256" bodyend="261"/>
        <references refid="classmlpack_1_1rl_1_1SumTree_1a326fbc401980f2b071d3a0f37d51be05" compoundref="sumtree_8hpp" startline="75" endline="86">SumTree&lt; T &gt;::BatchUpdate</references>
        <referencedby refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a9f4cfe4f0266408c291e51db0606b8e0" compoundref="prioritized__replay_8hpp" startline="289" endline="305">PrioritizedReplay&lt; EnvironmentType &gt;::Update</referencedby>
      </memberdef>
      </sectiondef>
    <briefdescription>
<para>Implementation of prioritized experience replay. </para>
    </briefdescription>
    <detaileddescription>
<para>Prioritized experience replay can replay important transitions more frequently by prioritizing transitions, and make agent learn more efficiently.</para>
<para><programlisting><codeline><highlight class="normal">@article{schaul2015prioritized,</highlight></codeline>
<codeline><highlight class="normal"><sp/>title<sp/><sp/><sp/>=<sp/>{Prioritized<sp/>experience<sp/>replay},</highlight></codeline>
<codeline><highlight class="normal"><sp/>author<sp/><sp/>=<sp/>{Schaul,<sp/>Tom<sp/>and<sp/>Quan,<sp/>John<sp/>and<sp/>Antonoglou,</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Ioannis<sp/>and<sp/>Silver,<sp/>David},</highlight></codeline>
<codeline><highlight class="normal"><sp/>journal<sp/>=<sp/>{arXiv<sp/>preprint<sp/>arXiv:1511.05952},</highlight></codeline>
<codeline><highlight class="normal"><sp/>year<sp/><sp/><sp/><sp/>=<sp/>{2015}</highlight></codeline>
<codeline><highlight class="normal"><sp/>}</highlight></codeline>
</programlisting></para>
<para><parameterlist kind="templateparam"><parameteritem>
<parameternamelist>
<parametername>EnvironmentType</parametername>
</parameternamelist>
<parameterdescription>
<para>Desired task. </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
    </detaileddescription>
    <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" line="39" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/replay/prioritized_replay.hpp" bodystart="40" bodyend="368"/>
    <listofallmembers>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a06e5cf57388e6c4657085808d2e78deb" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>actions</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1aaf7b2dc5d49d01961601c7c16be76777" prot="public" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>ActionType</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a20f4c4490bc8ecbdd1ffcb79acce6035" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>alpha</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1acc8b6b6879b002e34f544046de374ce8" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>batchSize</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a9424343761f8c4f4c1afe8f5b6bf471b" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>beta</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a26967aa9c873e7085b621d541d4120e0" prot="public" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>BetaAnneal</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1ad721fc6ca6a3d6ba3bc506576622aab0" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>capacity</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a19f31d9744c12503a9cbdaa1fd3a9848" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>full</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1abf36129b66f5b30a65d96d11ebfde027" prot="public" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>GetNStepInfo</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a690828f336f65c5372058811d4425337" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>idxSum</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a684edff0823aece7d245190fa8ffa2f3" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>initialBeta</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a76e2d6022b6c86f6365a2d6833840370" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>isTerminal</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a0bd142a7ea09a9d1070a4c3af7a355c2" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>maxPriority</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a0789a262d579655e5853c5d86e5b7632" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>nextStates</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1ada94392db6b18c65cb4406977697d429" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>nStepBuffer</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a48a86a6254329a98e1f15d4722c4e85b" prot="public" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>NSteps</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1ae87d0d3fe7950c32ba7d8f2715e84915" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>nSteps</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a7a04afe5347934be732ec70a70bd0a28" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>position</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a2d2ee6b689ad5f996c939be2f1f61ba0" prot="public" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>PrioritizedReplay</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a2754147b888db0a76084538e3426c4f9" prot="public" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>PrioritizedReplay</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a052b0363ff8606d45496f2d5016ac4b3" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>replayBetaIters</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a6c7dee98c84287c38d7ef6b8cc630dfd" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>rewards</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a6ecc6da2d5f83f0eefdc74be3465925a" prot="public" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>Sample</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1aeaa586e11d5a59a23ba5689667c447a6" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>sampledIndices</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a1a45c1e17aad599a64fa6f941979ad10" prot="public" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>SampleProportional</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1ab8983dc8f7847b4c77148b86d0e7fc8d" prot="public" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>Size</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a6a63dc9a374f1be6a4166445905c6c80" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>states</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1ada68ef405b7c331a2bee337614f00088" prot="public" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>StateType</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1ab17ee90540cf7b26647b57acf16116d5" prot="public" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>Store</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a9f4cfe4f0266408c291e51db0606b8e0" prot="public" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>Update</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a3b512739fae1601beafbb89d51c40a7d" prot="public" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>UpdatePriorities</name></member>
      <member refid="classmlpack_1_1rl_1_1PrioritizedReplay_1a36b9d4671b1f33385b9b80c2d654d5ff" prot="private" virt="non-virtual"><scope>mlpack::rl::PrioritizedReplay</scope><name>weights</name></member>
    </listofallmembers>
  </compounddef>
</doxygen>
