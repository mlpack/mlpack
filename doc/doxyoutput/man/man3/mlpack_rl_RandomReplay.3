.TH "RandomReplay< EnvironmentType >" 3 "Sun Jun 20 2021" "Version 3.4.2" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
RandomReplay< EnvironmentType > \- Implementation of random experience replay\&.  

.SH SYNOPSIS
.br
.PP
.SS "Classes"

.in +1c
.ti -1c
.RI "struct \fBTransition\fP"
.br
.in -1c
.SS "Public Types"

.in +1c
.ti -1c
.RI "using \fBActionType\fP = typename EnvironmentType::Action"
.br
.RI "Convenient typedef for action\&. "
.ti -1c
.RI "using \fBStateType\fP = typename EnvironmentType::State"
.br
.RI "Convenient typedef for state\&. "
.in -1c
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBRandomReplay\fP ()"
.br
.ti -1c
.RI "\fBRandomReplay\fP (const size_t batchSize, const size_t capacity, const size_t nSteps=1, const size_t dimension=StateType::dimension)"
.br
.RI "Construct an instance of random experience replay class\&. "
.ti -1c
.RI "void \fBGetNStepInfo\fP (double &reward, \fBStateType\fP &nextState, bool &isEnd, const double &discount)"
.br
.RI "Get the reward, next state and terminal boolean for nth step\&. "
.ti -1c
.RI "const size_t & \fBNSteps\fP () const"
.br
.RI "Get the number of steps for n-step agent\&. "
.ti -1c
.RI "void \fBSample\fP (arma::mat &sampledStates, std::vector< \fBActionType\fP > &sampledActions, arma::rowvec &sampledRewards, arma::mat &sampledNextStates, arma::irowvec &isTerminal)"
.br
.RI "Sample some experiences\&. "
.ti -1c
.RI "const size_t & \fBSize\fP ()"
.br
.RI "Get the number of transitions in the memory\&. "
.ti -1c
.RI "void \fBStore\fP (\fBStateType\fP state, \fBActionType\fP action, double reward, \fBStateType\fP nextState, bool isEnd, const double &discount)"
.br
.RI "Store the given experience\&. "
.ti -1c
.RI "void \fBUpdate\fP (arma::mat, std::vector< \fBActionType\fP >, arma::mat, arma::mat &)"
.br
.RI "Update the priorities of transitions and Update the gradients\&. "
.in -1c
.SH "Detailed Description"
.PP 

.SS "template<typename EnvironmentType>
.br
class mlpack::rl::RandomReplay< EnvironmentType >"
Implementation of random experience replay\&. 

At each time step, interactions between the agent and the environment will be saved to a memory buffer\&. When necessary, we can simply sample previous experiences from the buffer to train the agent\&. Typically this would be a random sample and the memory will be a First-In-First-Out buffer\&.
.PP
For more information, see the following\&.
.PP
.PP
.nf
@phdthesis{lin1993reinforcement,
 title  = {Reinforcement learning for robots using neural networks},
 author = {Lin, Long-Ji},
 year   = {1993},
 school = {Fujitsu Laboratories Ltd}
}
.fi
.PP
.PP
\fBTemplate Parameters\fP
.RS 4
\fIEnvironmentType\fP Desired task\&. 
.RE
.PP

.PP
Definition at line 44 of file random_replay\&.hpp\&.
.SH "Member Typedef Documentation"
.PP 
.SS "using \fBActionType\fP =  typename EnvironmentType::Action"

.PP
Convenient typedef for action\&. 
.PP
Definition at line 48 of file random_replay\&.hpp\&.
.SS "using \fBStateType\fP =  typename EnvironmentType::State"

.PP
Convenient typedef for state\&. 
.PP
Definition at line 51 of file random_replay\&.hpp\&.
.SH "Constructor & Destructor Documentation"
.PP 
.SS "\fBRandomReplay\fP ()\fC [inline]\fP"

.PP
Definition at line 62 of file random_replay\&.hpp\&.
.SS "\fBRandomReplay\fP (const size_t batchSize, const size_t capacity, const size_t nSteps = \fC1\fP, const size_t dimension = \fCStateType::dimension\fP)\fC [inline]\fP"

.PP
Construct an instance of random experience replay class\&. 
.PP
\fBParameters\fP
.RS 4
\fIbatchSize\fP Number of examples returned at each sample\&. 
.br
\fIcapacity\fP Total memory size in terms of number of examples\&. 
.br
\fInSteps\fP Number of steps to look in the future\&. 
.br
\fIdimension\fP The dimension of an encoded state\&. 
.RE
.PP

.PP
Definition at line 78 of file random_replay\&.hpp\&.
.SH "Member Function Documentation"
.PP 
.SS "void GetNStepInfo (double & reward, \fBStateType\fP & nextState, bool & isEnd, const double & discount)\fC [inline]\fP"

.PP
Get the reward, next state and terminal boolean for nth step\&. 
.PP
\fBParameters\fP
.RS 4
\fIreward\fP Given reward\&. 
.br
\fInextState\fP Given next state\&. 
.br
\fIisEnd\fP Whether next state is terminal state\&. 
.br
\fIdiscount\fP The discount parameter\&. 
.RE
.PP

.PP
Definition at line 151 of file random_replay\&.hpp\&.
.PP
Referenced by RandomReplay< EnvironmentType >::Store()\&.
.SS "const size_t& NSteps () const\fC [inline]\fP"

.PP
Get the number of steps for n-step agent\&. 
.PP
Definition at line 228 of file random_replay\&.hpp\&.
.SS "void Sample (arma::mat & sampledStates, std::vector< \fBActionType\fP > & sampledActions, arma::rowvec & sampledRewards, arma::mat & sampledNextStates, arma::irowvec & isTerminal)\fC [inline]\fP"

.PP
Sample some experiences\&. 
.PP
\fBParameters\fP
.RS 4
\fIsampledStates\fP Sampled encoded states\&. 
.br
\fIsampledActions\fP Sampled actions\&. 
.br
\fIsampledRewards\fP Sampled rewards\&. 
.br
\fIsampledNextStates\fP Sampled encoded next states\&. 
.br
\fIisTerminal\fP Indicate whether corresponding next state is terminal state\&. 
.RE
.PP

.PP
Definition at line 183 of file random_replay\&.hpp\&.
.SS "const size_t& Size ()\fC [inline]\fP"

.PP
Get the number of transitions in the memory\&. 
.PP
\fBReturns\fP
.RS 4
Actual used memory size 
.RE
.PP

.PP
Definition at line 206 of file random_replay\&.hpp\&.
.SS "void Store (\fBStateType\fP state, \fBActionType\fP action, double reward, \fBStateType\fP nextState, bool isEnd, const double & discount)\fC [inline]\fP"

.PP
Store the given experience\&. 
.PP
\fBParameters\fP
.RS 4
\fIstate\fP Given state\&. 
.br
\fIaction\fP Given action\&. 
.br
\fIreward\fP Given reward\&. 
.br
\fInextState\fP Given next state\&. 
.br
\fIisEnd\fP Whether next state is terminal state\&. 
.br
\fIdiscount\fP The discount parameter\&. 
.RE
.PP

.PP
Definition at line 104 of file random_replay\&.hpp\&.
.PP
References RandomReplay< EnvironmentType >::GetNStepInfo()\&.
.SS "void Update (arma::mat, std::vector< \fBActionType\fP >, arma::mat, arma::mat &)\fC [inline]\fP"

.PP
Update the priorities of transitions and Update the gradients\&. 
.PP
\fBParameters\fP
.RS 4
\fI*\fP (target) The learned value 
.br
\fI*\fP (sampledActions) Agent's sampled action 
.br
\fI*\fP (nextActionValues) Agent's next action 
.br
\fI*\fP (gradients) The model's gradients 
.RE
.PP

.PP
Definition at line 219 of file random_replay\&.hpp\&.

.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.
