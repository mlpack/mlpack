.TH "LSTM< InputDataType, OutputDataType >" 3 "Sun Jun 20 2021" "Version 3.4.2" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
LSTM< InputDataType, OutputDataType > \- Implementation of the \fBLSTM\fP module class\&.  

.SH SYNOPSIS
.br
.PP
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBLSTM\fP ()"
.br
.RI "Create the \fBLSTM\fP object\&. "
.ti -1c
.RI "\fBLSTM\fP (const \fBLSTM\fP &layer)"
.br
.RI "Copy constructor\&. "
.ti -1c
.RI "\fBLSTM\fP (const size_t inSize, const size_t outSize, const size_t rho=std::numeric_limits< size_t >::max())"
.br
.RI "Create the \fBLSTM\fP layer object using the specified parameters\&. "
.ti -1c
.RI "\fBLSTM\fP (\fBLSTM\fP &&)"
.br
.RI "Move constructor\&. "
.ti -1c
.RI "template<typename InputType , typename ErrorType , typename GradientType > void \fBBackward\fP (const InputType &input, const ErrorType &gy, GradientType &g)"
.br
.RI "Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards trough f\&. "
.ti -1c
.RI "OutputDataType & \fBDelta\fP ()"
.br
.RI "Modify the delta\&. "
.ti -1c
.RI "OutputDataType const  & \fBDelta\fP () const"
.br
.RI "Get the delta\&. "
.ti -1c
.RI "template<typename InputType , typename OutputType > void \fBForward\fP (const InputType &input, OutputType &output)"
.br
.RI "Ordinary feed-forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f\&. "
.ti -1c
.RI "template<typename InputType , typename OutputType > void \fBForward\fP (const InputType &input, OutputType &output, OutputType &cellState, bool useCellState=false)"
.br
.RI "Ordinary feed-forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f\&. "
.ti -1c
.RI "OutputDataType & \fBGradient\fP ()"
.br
.RI "Modify the gradient\&. "
.ti -1c
.RI "OutputDataType const  & \fBGradient\fP () const"
.br
.RI "Get the gradient\&. "
.ti -1c
.RI "template<typename InputType , typename ErrorType , typename GradientType > void \fBGradient\fP (const InputType &input, const ErrorType &error, GradientType &gradient)"
.br
.ti -1c
.RI "size_t \fBInputShape\fP () const"
.br
.RI "Get the shape of the input\&. "
.ti -1c
.RI "size_t \fBInSize\fP () const"
.br
.RI "Get the number of input units\&. "
.ti -1c
.RI "\fBLSTM\fP & \fBoperator=\fP (const \fBLSTM\fP &layer)"
.br
.RI "Copy assignment operator\&. "
.ti -1c
.RI "\fBLSTM\fP & \fBoperator=\fP (\fBLSTM\fP &&layer)"
.br
.RI "Move assignment operator\&. "
.ti -1c
.RI "OutputDataType & \fBOutputParameter\fP ()"
.br
.RI "Modify the output parameter\&. "
.ti -1c
.RI "OutputDataType const  & \fBOutputParameter\fP () const"
.br
.RI "Get the output parameter\&. "
.ti -1c
.RI "size_t \fBOutSize\fP () const"
.br
.RI "Get the number of output units\&. "
.ti -1c
.RI "OutputDataType & \fBParameters\fP ()"
.br
.RI "Modify the parameters\&. "
.ti -1c
.RI "OutputDataType const  & \fBParameters\fP () const"
.br
.RI "Get the parameters\&. "
.ti -1c
.RI "void \fBReset\fP ()"
.br
.ti -1c
.RI "void \fBResetCell\fP (const size_t size)"
.br
.ti -1c
.RI "size_t & \fBRho\fP ()"
.br
.RI "Modify the maximum number of steps to backpropagate through time (BPTT)\&. "
.ti -1c
.RI "size_t \fBRho\fP () const"
.br
.RI "Get the maximum number of steps to backpropagate through time (BPTT)\&. "
.ti -1c
.RI "template<typename Archive > void \fBserialize\fP (Archive &ar, const uint32_t)"
.br
.RI "Serialize the layer\&. "
.ti -1c
.RI "size_t \fBWeightSize\fP () const"
.br
.RI "Get the size of the weights\&. "
.in -1c
.SH "Detailed Description"
.PP 

.SS "template<typename InputDataType = arma::mat, typename OutputDataType = arma::mat>
.br
class mlpack::ann::LSTM< InputDataType, OutputDataType >"
Implementation of the \fBLSTM\fP module class\&. 

The implementation corresponds to the following algorithm:
.PP
\begin{eqnarray} i &=& sigmoid(W \cdot x + W \cdot h + W \cdot c + b) \\ f &=& sigmoid(W \cdot x + W \cdot h + W \cdot c + b) \\ z &=& tanh(W \cdot x + W \cdot h + b) \\ c &=& f \odot c + i \odot z \\ o &=& sigmoid(W \cdot x + W \cdot h + W \cdot c + b) \\ h &=& o \odot tanh(c) \end{eqnarray}.PP
Note that if an \fBLSTM\fP layer is desired as the first layer of a neural network, an IdentityLayer should be added to the network as the first layer, and then the \fBLSTM\fP layer should be added\&.
.PP
For more information, see the following\&.
.PP
.PP
.nf
@article{Graves2013,
  author  = {Alex Graves and Abdel{-}rahman Mohamed and Geoffrey E\&. Hinton},
  title   = {Speech Recognition with Deep Recurrent Neural Networks},
  journal = CoRR},
  year    = {2013},
  url     = {http://arxiv\&.org/abs/1303\&.5778},
}
.fi
.PP
.PP
\fBSee also\fP
.RS 4
\fBFastLSTM\fP for a faster \fBLSTM\fP version which combines the calculation of the input, forget, output gates and hidden state in a single step\&.
.RE
.PP
\fBTemplate Parameters\fP
.RS 4
\fIInputDataType\fP Type of the input data (arma::colvec, arma::mat, arma::sp_mat or arma::cube)\&. 
.br
\fIOutputDataType\fP Type of the output data (arma::colvec, arma::mat, arma::sp_mat or arma::cube)\&. 
.RE
.PP

.PP
Definition at line 62 of file lstm\&.hpp\&.
.SH "Constructor & Destructor Documentation"
.PP 
.SS "\fBLSTM\fP ()"

.PP
Create the \fBLSTM\fP object\&. 
.SS "\fBLSTM\fP (const size_t inSize, const size_t outSize, const size_t rho = \fCstd::numeric_limits< size_t >::max()\fP)"

.PP
Create the \fBLSTM\fP layer object using the specified parameters\&. 
.PP
\fBParameters\fP
.RS 4
\fIinSize\fP The number of input units\&. 
.br
\fIoutSize\fP The number of output units\&. 
.br
\fIrho\fP Maximum number of steps to backpropagate through time (BPTT)\&. 
.RE
.PP

.SS "\fBLSTM\fP (const \fBLSTM\fP< InputDataType, OutputDataType > & layer)"

.PP
Copy constructor\&. 
.SS "\fBLSTM\fP (\fBLSTM\fP< InputDataType, OutputDataType > &&)"

.PP
Move constructor\&. 
.SH "Member Function Documentation"
.PP 
.SS "void Backward (const InputType & input, const ErrorType & gy, GradientType & g)"

.PP
Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards trough f\&. Using the results from the feed forward pass\&.
.PP
\fBParameters\fP
.RS 4
\fIinput\fP The propagated input activation\&. 
.br
\fIgy\fP The backpropagated error\&. 
.br
\fIg\fP The calculated gradient\&. 
.RE
.PP

.SS "OutputDataType& Delta ()\fC [inline]\fP"

.PP
Modify the delta\&. 
.PP
Definition at line 173 of file lstm\&.hpp\&.
.SS "OutputDataType const& Delta () const\fC [inline]\fP"

.PP
Get the delta\&. 
.PP
Definition at line 171 of file lstm\&.hpp\&.
.SS "void Forward (const InputType & input, OutputType & output)"

.PP
Ordinary feed-forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f\&. 
.PP
\fBParameters\fP
.RS 4
\fIinput\fP Input data used for evaluating the specified function\&. 
.br
\fIoutput\fP Resulting output activation\&. 
.RE
.PP

.SS "void Forward (const InputType & input, OutputType & output, OutputType & cellState, bool useCellState = \fCfalse\fP)"

.PP
Ordinary feed-forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f\&. 
.PP
\fBParameters\fP
.RS 4
\fIinput\fP Input data used for evaluating the specified function\&. 
.br
\fIoutput\fP Resulting output activation\&. 
.br
\fIcellState\fP Cell state of the \fBLSTM\fP\&. 
.br
\fIuseCellState\fP Use the cellState passed in the \fBLSTM\fP cell\&. 
.RE
.PP

.SS "OutputDataType& Gradient ()\fC [inline]\fP"

.PP
Modify the gradient\&. 
.PP
Definition at line 178 of file lstm\&.hpp\&.
.SS "OutputDataType const& Gradient () const\fC [inline]\fP"

.PP
Get the gradient\&. 
.PP
Definition at line 176 of file lstm\&.hpp\&.
.SS "void Gradient (const InputType & input, const ErrorType & error, GradientType & gradient)"

.SS "size_t InputShape () const\fC [inline]\fP"

.PP
Get the shape of the input\&. 
.PP
Definition at line 193 of file lstm\&.hpp\&.
.SS "size_t InSize () const\fC [inline]\fP"

.PP
Get the number of input units\&. 
.PP
Definition at line 181 of file lstm\&.hpp\&.
.SS "\fBLSTM\fP& operator= (const \fBLSTM\fP< InputDataType, OutputDataType > & layer)"

.PP
Copy assignment operator\&. 
.SS "\fBLSTM\fP& operator= (\fBLSTM\fP< InputDataType, OutputDataType > && layer)"

.PP
Move assignment operator\&. 
.SS "OutputDataType& OutputParameter ()\fC [inline]\fP"

.PP
Modify the output parameter\&. 
.PP
Definition at line 168 of file lstm\&.hpp\&.
.SS "OutputDataType const& OutputParameter () const\fC [inline]\fP"

.PP
Get the output parameter\&. 
.PP
Definition at line 166 of file lstm\&.hpp\&.
.SS "size_t OutSize () const\fC [inline]\fP"

.PP
Get the number of output units\&. 
.PP
Definition at line 184 of file lstm\&.hpp\&.
.SS "OutputDataType& Parameters ()\fC [inline]\fP"

.PP
Modify the parameters\&. 
.PP
Definition at line 163 of file lstm\&.hpp\&.
.SS "OutputDataType const& Parameters () const\fC [inline]\fP"

.PP
Get the parameters\&. 
.PP
Definition at line 161 of file lstm\&.hpp\&.
.SS "void Reset ()"

.SS "void ResetCell (const size_t size)"

.SS "size_t& Rho ()\fC [inline]\fP"

.PP
Modify the maximum number of steps to backpropagate through time (BPTT)\&. 
.PP
Definition at line 158 of file lstm\&.hpp\&.
.SS "size_t Rho () const\fC [inline]\fP"

.PP
Get the maximum number of steps to backpropagate through time (BPTT)\&. 
.PP
Definition at line 156 of file lstm\&.hpp\&.
.SS "void serialize (Archive & ar, const uint32_t)"

.PP
Serialize the layer\&. 
.SS "size_t WeightSize () const\fC [inline]\fP"

.PP
Get the size of the weights\&. 
.PP
Definition at line 187 of file lstm\&.hpp\&.

.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.
