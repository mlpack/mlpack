.TH "SAC< EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType >" 3 "Sun Jun 20 2021" "Version 3.4.2" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
SAC< EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType > \- Implementation of Soft Actor-Critic, a model-free off-policy actor-critic based deep reinforcement learning algorithm\&.  

.SH SYNOPSIS
.br
.PP
.SS "Public Types"

.in +1c
.ti -1c
.RI "using \fBActionType\fP = typename EnvironmentType::Action"
.br
.RI "Convenient typedef for action\&. "
.ti -1c
.RI "using \fBStateType\fP = typename EnvironmentType::State"
.br
.RI "Convenient typedef for state\&. "
.in -1c
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBSAC\fP (\fBTrainingConfig\fP &config, QNetworkType &learningQ1Network, PolicyNetworkType &policyNetwork, ReplayType &replayMethod, UpdaterType qNetworkUpdater=UpdaterType(), UpdaterType policyNetworkUpdater=UpdaterType(), EnvironmentType environment=EnvironmentType())"
.br
.RI "Create the \fBSAC\fP object with given settings\&. "
.ti -1c
.RI "\fB~SAC\fP ()"
.br
.RI "Clean memory\&. "
.ti -1c
.RI "const \fBActionType\fP & \fBAction\fP () const"
.br
.RI "Get the action of the agent\&. "
.ti -1c
.RI "bool & \fBDeterministic\fP ()"
.br
.RI "Modify the training mode / test mode indicator\&. "
.ti -1c
.RI "const bool & \fBDeterministic\fP () const"
.br
.RI "Get the indicator of training mode / test mode\&. "
.ti -1c
.RI "double \fBEpisode\fP ()"
.br
.RI "Execute an episode\&. "
.ti -1c
.RI "void \fBSelectAction\fP ()"
.br
.RI "Select an action, given an agent\&. "
.ti -1c
.RI "void \fBSoftUpdate\fP (double rho)"
.br
.RI "Softly update the learning Q network parameters to the target Q network parameters\&. "
.ti -1c
.RI "\fBStateType\fP & \fBState\fP ()"
.br
.RI "Modify the state of the agent\&. "
.ti -1c
.RI "const \fBStateType\fP & \fBState\fP () const"
.br
.RI "Get the state of the agent\&. "
.ti -1c
.RI "size_t & \fBTotalSteps\fP ()"
.br
.RI "Modify total steps from beginning\&. "
.ti -1c
.RI "const size_t & \fBTotalSteps\fP () const"
.br
.RI "Get total steps from beginning\&. "
.ti -1c
.RI "void \fBUpdate\fP ()"
.br
.RI "Update the Q and policy networks\&. "
.in -1c
.SH "Detailed Description"
.PP 

.SS "template<typename EnvironmentType, typename QNetworkType, typename PolicyNetworkType, typename UpdaterType, typename ReplayType = RandomReplay<EnvironmentType>>
.br
class mlpack::rl::SAC< EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType >"
Implementation of Soft Actor-Critic, a model-free off-policy actor-critic based deep reinforcement learning algorithm\&. 

For more details, see the following: 
.PP
.nf
@misc{haarnoja2018soft,
 author    = {Tuomas Haarnoja and
              Aurick Zhou and
              Kristian Hartikainen and
              George Tucker and
              Sehoon Ha and
              Jie Tan and
              Vikash Kumar and
              Henry Zhu and
              Abhishek Gupta and
              Pieter Abbeel and
              Sergey Levine},
 title     = {Soft Actor-Critic Algorithms and Applications},
 year      = {2018},
 url       = {https://arxiv\&.org/abs/1812\&.05905}
}

.fi
.PP
.PP
\fBTemplate Parameters\fP
.RS 4
\fIEnvironmentType\fP The environment of the reinforcement learning task\&. 
.br
\fINetworkType\fP The network to compute action value\&. 
.br
\fIUpdaterType\fP How to apply gradients when training\&. 
.br
\fIReplayType\fP Experience replay method\&. 
.RE
.PP

.PP
Definition at line 63 of file sac\&.hpp\&.
.SH "Member Typedef Documentation"
.PP 
.SS "using \fBActionType\fP =  typename EnvironmentType::Action"

.PP
Convenient typedef for action\&. 
.PP
Definition at line 70 of file sac\&.hpp\&.
.SS "using \fBStateType\fP =  typename EnvironmentType::State"

.PP
Convenient typedef for state\&. 
.PP
Definition at line 67 of file sac\&.hpp\&.
.SH "Constructor & Destructor Documentation"
.PP 
.SS "\fBSAC\fP (\fBTrainingConfig\fP & config, QNetworkType & learningQ1Network, PolicyNetworkType & policyNetwork, ReplayType & replayMethod, UpdaterType qNetworkUpdater = \fCUpdaterType()\fP, UpdaterType policyNetworkUpdater = \fCUpdaterType()\fP, EnvironmentType environment = \fCEnvironmentType()\fP)"

.PP
Create the \fBSAC\fP object with given settings\&. If you want to pass in a parameter and discard the original parameter object, you can directly pass the parameter, as the constructor takes a reference\&. This avoids unnecessary copy\&.
.PP
\fBParameters\fP
.RS 4
\fIconfig\fP Hyper-parameters for training\&. 
.br
\fIlearningQ1Network\fP The network to compute action value\&. 
.br
\fIpolicyNetwork\fP The network to produce an action given a state\&. 
.br
\fIreplayMethod\fP Experience replay method\&. 
.br
\fIqNetworkUpdater\fP How to apply gradients to Q network when training\&. 
.br
\fIpolicyNetworkUpdater\fP How to apply gradients to policy network when training\&. 
.br
\fIenvironment\fP Reinforcement learning task\&. 
.RE
.PP

.SS "~\fBSAC\fP ()"

.PP
Clean memory\&. 
.SH "Member Function Documentation"
.PP 
.SS "const \fBActionType\fP& Action () const\fC [inline]\fP"

.PP
Get the action of the agent\&. 
.PP
Definition at line 136 of file sac\&.hpp\&.
.SS "bool& Deterministic ()\fC [inline]\fP"

.PP
Modify the training mode / test mode indicator\&. 
.PP
Definition at line 139 of file sac\&.hpp\&.
.SS "const bool& Deterministic () const\fC [inline]\fP"

.PP
Get the indicator of training mode / test mode\&. 
.PP
Definition at line 141 of file sac\&.hpp\&.
.SS "double Episode ()"

.PP
Execute an episode\&. 
.PP
\fBReturns\fP
.RS 4
Return of the episode\&. 
.RE
.PP

.SS "void SelectAction ()"

.PP
Select an action, given an agent\&. 
.SS "void SoftUpdate (double rho)"

.PP
Softly update the learning Q network parameters to the target Q network parameters\&. 
.PP
\fBParameters\fP
.RS 4
\fIrho\fP How 'softly' should the parameters be copied\&. 
.RE
.PP

.SS "\fBStateType\fP& State ()\fC [inline]\fP"

.PP
Modify the state of the agent\&. 
.PP
Definition at line 131 of file sac\&.hpp\&.
.SS "const \fBStateType\fP& State () const\fC [inline]\fP"

.PP
Get the state of the agent\&. 
.PP
Definition at line 133 of file sac\&.hpp\&.
.SS "size_t& TotalSteps ()\fC [inline]\fP"

.PP
Modify total steps from beginning\&. 
.PP
Definition at line 126 of file sac\&.hpp\&.
.SS "const size_t& TotalSteps () const\fC [inline]\fP"

.PP
Get total steps from beginning\&. 
.PP
Definition at line 128 of file sac\&.hpp\&.
.SS "void Update ()"

.PP
Update the Q and policy networks\&. 

.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.
